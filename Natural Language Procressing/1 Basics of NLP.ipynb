{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of NLP\n",
    "## Topics covered:\n",
    "1. Tokenization\n",
    "2. Removing stop words\n",
    "3. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\programdata\\anaconda3\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (1.19.2)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (3.3.2)\n",
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (8.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2020.6.20)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "import nltk       #natural language tool kit\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "## Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer, WordPunctTokenizer, TreebankWordTokenizer\n",
    "sentence_1 = \"ram, shyam and bob are good boys. hello world. bob's a nice boy.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence tokenization\n",
    "### Delimiters = fullstops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens =  ['ram, shyam and bob are good boys.', 'hello world.', \"bob's a nice boy.\"]\n",
      "number of tokens =  3\n"
     ]
    }
   ],
   "source": [
    "#tokens = sentence\n",
    "sentence_tokens = nltk.sent_tokenize(sentence_1)\n",
    "\n",
    "print(\"tokens = \",sentence_tokens)\n",
    "print(\"number of tokens = \",len(sentence_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## White space tokenization\n",
    "### Delimiter = white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens =  ['ram,', 'shyam', 'and', 'bob', 'are', 'good', 'boys.', 'hello', 'world.', \"bob's\", 'a', 'nice', 'boy.']\n",
      "number of tokens =  13\n"
     ]
    }
   ],
   "source": [
    "tk = WhitespaceTokenizer()\n",
    "whitespace_tokens = tk.tokenize(sentence_1)\n",
    "\n",
    "print(\"tokens = \",whitespace_tokens)\n",
    "print(\"number of tokens = \",len(whitespace_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word punctuation tokenizer\n",
    "### Seperates punctuation from words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens =  ['ram', ',', 'shyam', 'and', 'bob', 'are', 'good', 'boys', '.', 'hello', 'world', '.', 'bob', \"'\", 's', 'a', 'nice', 'boy', '.']\n",
      "number of tokens =  19\n"
     ]
    }
   ],
   "source": [
    "tk = WordPunctTokenizer()\n",
    "word_punctuation_tokens = tk.tokenize(sentence_1)\n",
    "\n",
    "print(\"tokens = \",word_punctuation_tokens)\n",
    "print(\"number of tokens = \",len(word_punctuation_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree bank Word Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens =  ['ram', ',', 'shyam', 'and', 'bob', 'are', 'good', 'boys.', 'hello', 'world.', 'bob', \"'s\", 'a', 'nice', 'boy', '.']\n",
      "number of tokens =  16\n"
     ]
    }
   ],
   "source": [
    "tk = TreebankWordTokenizer()\n",
    "tree_bank_tokens = tk.tokenize(sentence_1)\n",
    "\n",
    "print(\"tokens = \",tree_bank_tokens)\n",
    "print(\"number of tokens = \",len(tree_bank_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token normalization\n",
    "## Text normalization is the process of transforming a text into a canonical form. That is, bringing a sentence to a predefined standard. it can be done in two ways,\n",
    "\n",
    "### 1. Stemming\n",
    "### 2. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words\n",
    "\n",
    "## Before we normalisze, we need to get rid of the stop words, stop words are words that are common in any language like 'the', 'and' etc which dont add value to the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop words count =  179\n",
      "stop words in english are : \n",
      "\n",
      "{'should', 'during', 'was', \"didn't\", 'why', 'y', 'a', 'few', 'them', 'yourselves', 'out', \"mightn't\", 'after', 'is', 'can', 'by', 'me', 'd', 'ain', 'does', 'will', 'on', 'be', 'further', 'themselves', \"weren't\", 'couldn', 'it', 'hadn', 'and', 'below', 'then', \"hadn't\", \"shouldn't\", \"you've\", \"doesn't\", 'from', 'once', 'ourselves', 'up', 'are', \"you'll\", \"you're\", 'himself', 'were', 'any', \"you'd\", \"aren't\", 'yours', 'between', 'our', 'if', 'has', 'but', 'of', 'her', 'his', 'over', 'again', 'myself', 'under', 're', 'while', 'don', 'such', 'both', \"shan't\", 'more', 'isn', 'aren', 'weren', 'most', 'do', 'am', 'whom', \"couldn't\", 'o', 'here', \"wouldn't\", 'this', \"mustn't\", 'each', 'so', 'through', 'who', 'mightn', \"it's\", 's', 'shan', 'ours', 'what', 'wasn', \"hasn't\", 'itself', 'very', 'same', 'shouldn', \"don't\", 'nor', 'll', 'some', 'doing', 'doesn', 'too', 'its', 'about', 'when', \"should've\", 'you', 'hers', 'until', \"needn't\", 'your', 'their', 'just', 'being', 't', 'mustn', 'off', 'because', 'down', 'she', 'that', 'these', 'now', 'own', 'for', 'ma', 'an', \"haven't\", 'hasn', 'before', \"that'll\", 'against', 'there', 've', 'him', 'he', 'haven', 'all', 'i', 'herself', 'than', 'how', 'have', 'had', 'where', 'no', 'm', 'at', 'only', \"wasn't\", 'which', 'in', 'didn', 'with', 'to', 'been', 'yourself', 'or', 'those', 'needn', 'wouldn', 'other', \"won't\", 'did', 'we', 'into', 'theirs', 'above', \"isn't\", 'as', 'not', 'my', 'they', \"she's\", 'won', 'the', 'having'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(\"stop words count = \", len(stop_words))\n",
    "print(\"stop words in english are : \\n\")\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with tokens    ::  ['ram,', 'shyam', 'and', 'bob', 'are', 'good', 'boys.', 'hello', 'world.', \"bob's\", 'a', 'nice', 'boy.']\n",
      "without tokens ::  ['ram,', 'shyam', 'bob', 'good', 'boys.', 'hello', 'world.', \"bob's\", 'nice', 'boy.']\n"
     ]
    }
   ],
   "source": [
    "# removing stop words\n",
    "whitespace_tokens_op = [w for w in whitespace_tokens if not w in stop_words]\n",
    "print(\"with tokens    :: \",whitespace_tokens)\n",
    "print(\"without tokens :: \",whitespace_tokens_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "## Stemming is the procress of removing or replacing the suffix of a word to get the root words. for example,\n",
    "### wolf, wolves -> wolf\n",
    "### talk, talks  -> talk\n",
    "### bob, bob's   -> bob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
